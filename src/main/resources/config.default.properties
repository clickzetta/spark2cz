# Source connector specific configurations
# NOTE: All custom configurations MUST start with 'spark.' prefix to avoid being filtered out by Spark
## doris table spark config key mapping
spark.source.table.key=doris.table.identifier
## multi-table format: db.table1,db.table2
spark.source.table.values=peisong_smv.smv_agg_60049975_60049979_1747795281474_uhza
## doris format
spark.source.format=org.apache.doris.spark.sql.sources.DorisDataSource

# doris connector Configuration
## doris FE host
spark.source.connector.doris.fenodes=compar*******i.com:8410
## doris username
spark.source.connector.doris.user=peison*******_cluster
## doris password
spark.source.connector.doris.password=*******
## doris connection config
spark.source.connector.doris.request.retries=3
spark.source.connector.doris.request.timeout=60000
spark.source.connector.doris.request.batch.size=1024
spark.source.connector.doris.exec.mem.limit=4294967296
spark.source.connector.doris.request.tablet.size=10
## doris read bitmap type as string type, convert bitmap to string when reading from doris
## use binary_to_bitmap and string_to_binary() functions in studio for conversion, create table as select ... sql syntax
spark.source.connector.doris.read.bitmap-to-string=true
spark.source.connector.doris.filter.query=dt between '2022-01-01' and '2022-12-31'

# Spark layer data filter (optional, similar to doris filter.query but for Spark SQL)
# If specified, will add WHERE clause to the insert SQL
spark.filter.query=dt between '2022-01-01' and '2022-12-31'

# Clickzetta Configuration
## database http connection url, url path with &use_http=true
spark.clickzetta.sdk.url=jdbc:clickzetta://9f8424ae.host:8033/quick_start?username=mt_admin&password=******%23&schema=peisong_smv&virtualCluster=default&use_http=true
spark.clickzetta.sdk.schema=peisong_smv
spark.clickzetta.sdk.format=com.clickzetta.spark.clickzetta.ClickzettaRelationProvider
spark.clickzetta.sdk.access_mode=studio
spark.clickzetta.sdk.use_http=true
## data verification switch, default false
spark.clickzetta.sdk.enable.results.verify=false
## concurrent execution switch, default false
spark.clickzetta.sdk.enable.concurrent.copy=false
## data save mode, supports append/overwrite/errorifexists/ignore, default overwrite
spark.clickzetta.sdk.save.mode=overwrite
## bitmap cast to binary switch, default false
spark.clickzetta.sdk.enable.bitmap_to_binary=false
## configure bulkload preferred options
spark.clickzetta.sdk.cz.bulkload.prefer.internal.endpoint=false
## local temp directory
spark.clickzetta.sdk.cz.bulkload.load.uri=file:///data1/bulkload-temp
## data format json, default is parquet
#spark.clickzetta.sdk.cz.bulkload.load.format=json
## configure clickzetta bulkload cleanup strategy parameters
#spark.clickzetta.sdk.cz.bulkload.file.purge.enable=false
#spark.clickzetta.sdk.cz.bulkload.merge.into.table.volume.cleanup.enable=false
#spark.clickzetta.sdk.cz.bulkload.local.file.cleanup.enable=false
spark.clickzetta.sdk.spark.cz.bulkload.sql.cz.sql.table.sink.supports.eq=false
## choose bulkload v1 or v2
#spark.clickzetta.sdk.spark.cz.writeVersion=V1

# Spark Configuration
spark.eventLog.enabled=true
spark.eventLog.dir=file:///data1/bi_doris/spark_event_logs
spark.sql.shuffle.partitions=500
spark.default.parallelism=500
spark.rdd.compress=true
spark.executor.instances=50
spark.executor.memoryOverhead=2g
## temp directory - configure Spark temp directory
spark.local.dir=/data1/spark-temp
## local temp directory config
spark.driver.extraJavaOptions=-Djava.io.tmpdir=/data1/java-temp
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/data1/java-temp
## Hadoop temp directory
hadoop.tmp.dir=/data1/hadoop-temp
## SQL warehouse directory
spark.sql.warehouse.dir=/data1/spark-warehouse